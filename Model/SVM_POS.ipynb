{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_POS_data.csv')\n",
    "function = lambda s: [s[\"word\"].values.tolist(), s[\"tag\"].values.tolist()]\n",
    "grouped = df.groupby(\"sentence_id\").apply(function)\n",
    "items = [s for s in grouped]\n",
    "final_df = pd.DataFrame(items,columns = ['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = final_df['x'].tolist()\n",
    "poss = final_df['y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [\"\" for i in range(int(WINDOW)//2)] + sentences[i] + [\"\" for i in range(int(WINDOW)//2)]\n",
    "    poss[i] = [\"PAD\" for i in range(int(WINDOW)//2)] + poss[i] + [\"PAD\" for i in range(int(WINDOW)//2)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = sentences[:int(len(sentences)*0.7)]\n",
    "val_sents = sentences[int(len(sentences)*0.7):int(len(sentences)*0.9)]\n",
    "test_sents = sentences[int(len(sentences)*0.9):]\n",
    "assert len(train_sents)+len(val_sents)+len(test_sents) == len(sentences)\n",
    "train_pos = poss[:int(len(poss)*0.7)]\n",
    "val_pos = poss[int(len(poss)*0.7):int(len(poss)*0.9)]\n",
    "test_pos = poss[int(len(poss)*0.9):]\n",
    "assert len(train_pos)+len(val_pos)+len(test_pos) == len(poss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_train_val = train_sents.copy()\n",
    "sents_train_val.extend(val_sents)\n",
    "pos_train_val = train_pos.copy()\n",
    "pos_train_val.extend(val_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = []\n",
    "for tags in pos_train_val:\n",
    "    for tag in tags:\n",
    "        if tag not in tags_list:\n",
    "            tags_list.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = []\n",
    "for words in sents_train_val:\n",
    "    for word in words:\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "word_tags = {}\n",
    "for i in words_list:\n",
    "    tags = {}\n",
    "    for j in tags_list:\n",
    "        tags[j] = 0\n",
    "    word_tags[i] = tags\n",
    "    word_freq[i] = 0\n",
    "for i in range(len(sents_train_val)):\n",
    "    for j in range(len(sents_train_val[i])):\n",
    "        word_freq[sents_train_val[i][j]] += 1\n",
    "        word_tags[sents_train_val[i][j]][pos_train_val[i][j]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguities = {}\n",
    "for word in words_list:\n",
    "    ambiguities[word] = [j for j in word_tags[word] if word_tags[word][j] !=0]\n",
    "    assert word_freq[word] == sum(word_tags[word].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = gensim.models.Word2Vec.load('full_grams_cbow_100_twitter.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoder(number, lenght):\n",
    "    zero = np.zeros(lenght)\n",
    "    zero[number] = 1\n",
    "    return zero\n",
    "\n",
    "def getFeatures(wordIdx, sentence, pos, w2v, tags, ambiguities, train=True):\n",
    "    features = []\n",
    "    \n",
    "    keys = w2v.wv.key_to_index.keys()\n",
    "    for i in reversed(range(1,int(WINDOW)//2+1)):\n",
    "        if sentence[wordIdx-i] not in keys:\n",
    "            features.append(np.zeros(w2v.vector_size))\n",
    "        else:\n",
    "            features.append(w2v.wv.get_vector(sentence[wordIdx-i], norm=True))\n",
    "    \n",
    "    if sentence[wordIdx] not in keys:\n",
    "        features.append(np.zeros(w2v.vector_size))\n",
    "    else:\n",
    "        features.append(w2v.wv.get_vector(sentence[wordIdx], norm=True))\n",
    "    \n",
    "    for i in range(1,int(WINDOW)//2+1):\n",
    "        if sentence[wordIdx+i] not in keys:\n",
    "            features.append(np.zeros(w2v.vector_size))\n",
    "        else:\n",
    "            features.append(w2v.wv.get_vector(sentence[wordIdx+i], norm=True))\n",
    "    if train:\n",
    "        for i in reversed(range(1,int(WINDOW)//2+1)):\n",
    "            tag = pos[wordIdx-i]\n",
    "            features.append(OneHotEncoder(tags.index(tag),len(tags)))\n",
    "            \n",
    "        if sentence[wordIdx] in ambiguities:\n",
    "            features.append(OneHotEncoder([tags.index(i) for i in ambiguities[sentence[wordIdx]]],len(tags)))\n",
    "        else:\n",
    "            features.append(OneHotEncoder([],len(tags)))\n",
    "\n",
    "    else:\n",
    "        for i in reversed(range(1,int(WINDOW)//2+1)):\n",
    "            tag = pos[wordIdx-i]\n",
    "            features.append(OneHotEncoder(tags.index(tag),len(tags)))\n",
    "            \n",
    "        if sentence[wordIdx] in ambiguities:\n",
    "            features.append(OneHotEncoder([tags.index(i) for i in ambiguities[sentence[wordIdx]]],len(tags)))\n",
    "        else:\n",
    "            features.append(OneHotEncoder([],len(tags)))\n",
    "    \n",
    "    features.append([len(sentence[wordIdx])])\n",
    "    \n",
    "    flat_list = []\n",
    "    for i in features:\n",
    "        flat_list.extend(i)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in tqdm(range(len(train_sents))):\n",
    "    for j in range(int(WINDOW)//2,len(train_sents[i]) - int(WINDOW)//2):\n",
    "        x_train.append(getFeatures(j , train_sents[i], train_pos[i], embedding_model, tags_list, ambiguities,train=True))\n",
    "        y_train.append(train_pos[i][j])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "for i in tqdm(range(len(val_sents))):\n",
    "    for j in range(int(WINDOW)//2,len(val_sents[i]) - int(WINDOW)//2):\n",
    "        x_val.append(getFeatures(j , val_sents[i], val_pos[i], embedding_model, tags_list, ambiguities))\n",
    "        y_val.append(val_pos[i][j])\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "y_val = encoder.transform(y_val)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in tqdm(range(len(test_sents))):\n",
    "    for j in range(int(WINDOW)//2,len(test_sents[i]) - int(WINDOW)//2):\n",
    "        x_test.append(getFeatures(j , test_sents[i], test_pos[i], embedding_model, tags_list, ambiguities))\n",
    "        y_test.append(test_pos[i][j])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape (x y) \", x_train.shape,y_train.shape)\n",
    "print(\"Validation data shape (x y) \", x_val.shape,y_val.shape)\n",
    "print(\"Test data shape (x y) \", x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f1 = h5py.File(\"data.hdf5\", \"w\")\n",
    "dset1 = f1.create_dataset(\"x_train\", x_train.shape , dtype='f', data=x_train)\n",
    "dset1 = f1.create_dataset(\"y_train\", y_train.shape , dtype='i', data=y_train)\n",
    "dset1 = f1.create_dataset(\"x_val\", x_val.shape , dtype='f', data=x_val)\n",
    "dset1 = f1.create_dataset(\"y_val\", y_val.shape , dtype='i', data=y_val)\n",
    "dset1 = f1.create_dataset(\"x_test\", x_test.shape , dtype='f', data=x_test)\n",
    "dset1 = f1.create_dataset(\"y_test\", y_test.shape , dtype='i', data=y_test)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f2 = h5py.File('data.hdf5', 'r')\n",
    "x_train = f2['x_train']\n",
    "x_train = x_train[:]\n",
    "x_val = f2['x_val']\n",
    "x_val = x_val[:]\n",
    "x_test = f2['x_test']\n",
    "x_test = x_test[:]\n",
    "y_train = f2['y_train']\n",
    "y_train = y_train[:]\n",
    "y_val = f2['y_val']\n",
    "y_val = y_val[:]\n",
    "y_test = f2['y_test']\n",
    "y_test = y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (x y)  (832429, 606) (832429,)\n",
      "Validation data shape (x y)  (142833, 606) (142833,)\n",
      "Test data shape (x y)  (42477, 606) (42477,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape (x y) \", x_train.shape,y_train.shape)\n",
    "print(\"Validation data shape (x y) \", x_val.shape,y_val.shape)\n",
    "print(\"Test data shape (x y) \", x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = SVC(C=100)\n",
    "clf1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = clf1.predict(x_train)\n",
    "train_acc = sum(train_pred == y_train) *1.0 / len(train_pred)\n",
    "print('Train Acc',train_acc*100)\n",
    "train_f1 = f1_score(y_train, train_pred, average='macro')\n",
    "print('Train F1',train_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf1.predict(x_val)\n",
    "val_acc = sum(val_pred == y_val) *1.0 / len(val_pred)\n",
    "print('Val Acc',val_acc*100)\n",
    "val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "print('Val F1',val_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 93.76603809120229\n",
      "Test F1 76.64003553665913\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf1.predict(x_test)\n",
    "test_acc = sum(test_pred == y_test) *1.0 / len(test_pred)\n",
    "print('Test Acc',test_acc*100)\n",
    "test_f1 = f1_score(y_test, test_pred, average='macro')\n",
    "print('Test F1',test_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'svm_pos_tag.pickle'\n",
    "pickle.dump(clf1, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
