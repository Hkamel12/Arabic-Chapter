{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - EMBEDDINGS Gensim300 Test_Accuracy=93.61 F1=81.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "    #df=df.drop(columns=['Case','lemma', 'Gender','Number','State','Gloss','Proclitic 1','Proclitic 2','Proclitic 0','Aspect','Mood','Person','Voice','Enclitic 0'])\n",
    "    df=df.drop_duplicates()\n",
    "    df[\"POS\"] = df[\"POS\"].apply(lambda x: x.replace(\"Proper Noun\", \"ProperNoun\"))\n",
    "    df1 = pd.DataFrame(columns = ['x','y'])\n",
    "    for i in df['sentence_id'].unique():\n",
    "        df1 = df1.append({'x':df[df['sentence_id'] == i]['word'].tolist() , 'y':df[df['sentence_id'] == i]['POS'].tolist()}, ignore_index=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KUSHBAB\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py:3338: DtypeWarning: Columns (18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "csvs = os.listdir('Dataset/')\n",
    "final_df = pd.DataFrame()\n",
    "for csv in csvs:\n",
    "    df = process_csv('Dataset/'+csv)\n",
    "    final_df = pd.concat([final_df,df])\n",
    "final_df = final_df.sample(frac = 1)\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = final_df['x'].tolist()\n",
    "poss = final_df['y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [\"\" for i in range(int(WINDOW)//2)] + sentences[i] + [\"\" for i in range(int(WINDOW)//2)]\n",
    "    poss[i] = [\"PAD\" for i in range(int(WINDOW)//2)] + poss[i] + [\"PAD\" for i in range(int(WINDOW)//2)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = sentences[:int(len(sentences)*0.7)]\n",
    "val_sents = sentences[int(len(sentences)*0.7):int(len(sentences)*0.9)]\n",
    "test_sents = sentences[int(len(sentences)*0.9):]\n",
    "assert len(train_sents)+len(val_sents)+len(test_sents) == len(sentences)\n",
    "train_pos = poss[:int(len(poss)*0.7)]\n",
    "val_pos = poss[int(len(poss)*0.7):int(len(poss)*0.9)]\n",
    "test_pos = poss[int(len(poss)*0.9):]\n",
    "assert len(train_pos)+len(val_pos)+len(test_pos) == len(poss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_train_val = train_sents.copy()\n",
    "sents_train_val.extend(val_sents)\n",
    "pos_train_val = train_pos.copy()\n",
    "pos_train_val.extend(val_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = []\n",
    "for tags in pos_train_val:\n",
    "    for tag in tags:\n",
    "        if tag not in tags_list:\n",
    "            tags_list.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = []\n",
    "for words in sents_train_val:\n",
    "    for word in words:\n",
    "        if word not in words_list:\n",
    "            words_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "word_tags = {}\n",
    "for i in words_list:\n",
    "    tags = {}\n",
    "    for j in tags_list:\n",
    "        tags[j] = 0\n",
    "    word_tags[i] = tags\n",
    "    word_freq[i] = 0\n",
    "for i in range(len(sents_train_val)):\n",
    "    for j in range(len(sents_train_val[i])):\n",
    "        word_freq[sents_train_val[i][j]] += 1\n",
    "        word_tags[sents_train_val[i][j]][pos_train_val[i][j]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguities = {}\n",
    "for word in words_list:\n",
    "    ambiguities[word] = [j for j in word_tags[word] if word_tags[word][j] !=0]\n",
    "    assert word_freq[word] == sum(word_tags[word].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests, zipfile\n",
    "#from io import BytesIO\n",
    "#response = requests.get(\"https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_twitter.zip\")\n",
    "#zipDocument = zipfile.ZipFile(BytesIO(response.content))\n",
    "#zipDocument.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = gensim.models.Word2Vec.load('full_grams_cbow_300_twitter.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoder(number, lenght):\n",
    "    zero = np.zeros(lenght)\n",
    "    zero[number] = 1\n",
    "    return zero\n",
    "\n",
    "def getFeatures(wordIdx, sentence, pos, w2v, tags, ambiguities, train=True):\n",
    "    features = []\n",
    "    \n",
    "    keys = w2v.wv.key_to_index.keys()\n",
    "    for i in reversed(range(1,int(WINDOW)//2+1)):\n",
    "        if sentence[wordIdx-i] not in keys:\n",
    "            features.append(np.zeros(w2v.vector_size))\n",
    "        else:\n",
    "            features.append(w2v.wv.get_vector(sentence[wordIdx-i], norm=True))\n",
    "    \n",
    "    if sentence[wordIdx] not in keys:\n",
    "        features.append(np.zeros(w2v.vector_size))\n",
    "    else:\n",
    "        features.append(w2v.wv.get_vector(sentence[wordIdx], norm=True))\n",
    "    \n",
    "    for i in range(1,int(WINDOW)//2+1):\n",
    "        if sentence[wordIdx+i] not in keys:\n",
    "            features.append(np.zeros(w2v.vector_size))\n",
    "        else:\n",
    "            features.append(w2v.wv.get_vector(sentence[wordIdx+i], norm=True))\n",
    "    if train:\n",
    "        for i in reversed(range(1,int(WINDOW)//2+1)):\n",
    "            tag = pos[wordIdx-i]\n",
    "            features.append(OneHotEncoder(tags.index(tag),len(tags)))\n",
    "            \n",
    "        if sentence[wordIdx] in ambiguities:\n",
    "            features.append(OneHotEncoder([tags.index(i) for i in ambiguities[sentence[wordIdx]]],len(tags)))\n",
    "        else:\n",
    "            features.append(OneHotEncoder([],len(tags)))\n",
    "\n",
    "    else:\n",
    "        for i in reversed(range(1,int(WINDOW)//2+1)):\n",
    "            tag = pos[wordIdx-i]\n",
    "            features.append(OneHotEncoder(tags.index(tag),len(tags)))\n",
    "            \n",
    "        if sentence[wordIdx] in ambiguities:\n",
    "            features.append(OneHotEncoder([tags.index(i) for i in ambiguities[sentence[wordIdx]]],len(tags)))\n",
    "        else:\n",
    "            features.append(OneHotEncoder([],len(tags)))\n",
    "    \n",
    "    features.append([len(sentence[wordIdx])])\n",
    "    \n",
    "    flat_list = []\n",
    "    for i in features:\n",
    "        flat_list.extend(i)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10045/10045 [00:43<00:00, 231.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2870/2870 [00:08<00:00, 331.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1435/1435 [00:03<00:00, 420.80it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "for i in tqdm(range(len(train_sents))):\n",
    "    for j in range(int(WINDOW)//2,len(train_sents[i]) - int(WINDOW)//2):\n",
    "        x_train.append(getFeatures(j , train_sents[i], train_pos[i], embedding_model, tags_list, ambiguities))\n",
    "        y_train.append(train_pos[i][j])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "for i in tqdm(range(len(val_sents))):\n",
    "    for j in range(int(WINDOW)//2,len(val_sents[i]) - int(WINDOW)//2):\n",
    "        x_val.append(getFeatures(j , val_sents[i], val_pos[i], embedding_model, tags_list, ambiguities))\n",
    "        y_val.append(val_pos[i][j])\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in tqdm(range(len(test_sents))):\n",
    "    for j in range(int(WINDOW)//2,len(test_sents[i]) - int(WINDOW)//2):\n",
    "        x_test.append(getFeatures(j , test_sents[i], test_pos[i], embedding_model, tags_list, ambiguities))\n",
    "        y_test.append(test_pos[i][j])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (x y)  (181182, 1600) (181182,)\n",
      "Validation data shape (x y)  (52531, 1600) (52531,)\n",
      "Test data shape (x y)  (26013, 1600) (26013,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape (x y) \", x_train.shape,y_train.shape)\n",
    "print(\"Validation data shape (x y) \", x_val.shape,y_val.shape)\n",
    "print(\"Test data shape (x y) \", x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C = [1,10,100,1000]\n",
    "#degree = np.arange(1, 7)\n",
    "\n",
    "#params = {'C' : C,'degree' : degree}\n",
    "\n",
    "#random_search = RandomizedSearchCV(estimator = SVC(),param_distributions = params,n_iter = 50,n_jobs = 6,verbose=1).fit(x_train, y_train)\n",
    "\n",
    "#clf = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM  C=1.0 Kernel=rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = SVC(C=1)\n",
    "clf1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc 98.64445695488514\n",
      "Train F1 88.45780515174259\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf1.predict(x_train)\n",
    "train_acc = sum(train_pred == y_train) *1.0 / len(train_pred)\n",
    "print('Train Acc',train_acc*100)\n",
    "train_f1 = f1_score(y_train, train_pred, average='macro')\n",
    "print('Train F1',train_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc 98.53610249186194\n",
      "Val F1 88.71133609733224\n"
     ]
    }
   ],
   "source": [
    "val_pred = clf1.predict(x_val)\n",
    "val_acc = sum(val_pred == y_val) *1.0 / len(val_pred)\n",
    "print('Val Acc',val_acc*100)\n",
    "val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "print('Val F1',val_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 93.61088686426018\n",
      "Test F1 81.21378798536703\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf1.predict(x_test)\n",
    "test_acc = sum(test_pred == y_test) *1.0 / len(test_pred)\n",
    "print('Test Acc',test_acc*100)\n",
    "test_f1 = f1_score(y_test, test_pred, average='macro')\n",
    "print('Test F1',test_f1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM  C=10.0 Kernel=rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = SVC(C=10)\n",
    "clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc 99.44365334304732\n",
      "Train F1 97.71368176903825\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf2.predict(x_train)\n",
    "train_acc = sum(train_pred == y_train) *1.0 / len(train_pred)\n",
    "print('Train Acc',train_acc*100)\n",
    "train_f1 = f1_score(y_train, train_pred, average='macro')\n",
    "print('Train F1',train_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc 98.8482990995793\n",
      "Val F1 92.3807055667022\n"
     ]
    }
   ],
   "source": [
    "val_pred = clf2.predict(x_val)\n",
    "val_acc = sum(val_pred == y_val) *1.0 / len(val_pred)\n",
    "print('Val Acc',val_acc*100)\n",
    "val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "print('Val F1',val_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 93.9068927074924\n",
      "Test F1 80.33619100661515\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf2.predict(x_test)\n",
    "test_acc = sum(test_pred == y_test) *1.0 / len(test_pred)\n",
    "print('Test Acc',test_acc*100)\n",
    "test_f1 = f1_score(y_test, test_pred, average='macro')\n",
    "print('Test F1',test_f1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM  C=100.0 Kernel=rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = SVC(C=100)\n",
    "clf3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc 99.91941804373504\n",
      "Train F1 99.6846514498401\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf3.predict(x_train)\n",
    "train_acc = sum(train_pred == y_train) *1.0 / len(train_pred)\n",
    "print('Train Acc',train_acc*100)\n",
    "train_f1 = f1_score(y_train, train_pred, average='macro')\n",
    "print('Train F1',train_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc 98.69981534712836\n",
      "Val F1 92.41075503881393\n"
     ]
    }
   ],
   "source": [
    "val_pred = clf3.predict(x_val)\n",
    "val_acc = sum(val_pred == y_val) *1.0 / len(val_pred)\n",
    "print('Val Acc',val_acc*100)\n",
    "val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "print('Val F1',val_f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 93.54169069311497\n",
      "Test F1 80.79178399458982\n"
     ]
    }
   ],
   "source": [
    "test_pred = clf3.predict(x_test)\n",
    "test_acc = sum(test_pred == y_test) *1.0 / len(test_pred)\n",
    "print('Test Acc',test_acc*100)\n",
    "test_f1 = f1_score(y_test, test_pred, average='macro')\n",
    "print('Test F1',test_f1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best SVM C=1 kernel=rbf Test_accuracy=93.61 F1=81.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(w, sent):\n",
    "    words = sent.split()\n",
    "    words = [\"\" for i in range(int(w)//2)] + words + [\"\" for i in range(int(w)//2)]\n",
    "    pos = [\"PAD\" for i in range(int(w)//2)]\n",
    "    for i in range(int(WINDOW)//2,len(words) - int(WINDOW)//2):\n",
    "        feature = np.array(getFeatures(i , words, pos, embedding_model, tags_list, ambiguities, train = False)).reshape(1,-1)\n",
    "        tag = clf.predict(feature)\n",
    "        pos.append(tag[0])\n",
    "    return pos[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جون ProperNoun\n",
      "يحب Verb\n",
      "البيت Noun\n",
      "الأزرق Noun\n",
      "في Preposition\n",
      "نهاية Noun\n",
      "الشارع Noun\n"
     ]
    }
   ],
   "source": [
    "sentence = \"جون يحب البيت الأزرق في نهاية الشارع\"\n",
    "output = pos_tagger(WINDOW ,sentence)\n",
    "pred_tags = [(sentence.split()[i],output[i]) for i in range(len(sentence.split()))]\n",
    "for w,t in pred_tags:\n",
    "    print(w,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate_pred import *\n",
    "evaluate(sentence, pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SVM_POS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
