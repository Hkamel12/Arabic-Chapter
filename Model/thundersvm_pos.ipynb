{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thundersvm_pos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etdt_BaHcJRl",
        "outputId": "383cbc0e-5c70-48ad-ba42-e3f8fdaaaa61"
      },
      "source": [
        "! pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5TceidvuT_7",
        "outputId": "6b9df0b1-0d8c-4ee1-bb2c-006cd61849cf"
      },
      "source": [
        "! git clone https://github.com/Xtra-Computing/thundersvm.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'thundersvm' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6_ZnWqOxjU-",
        "outputId": "b17356ef-7ee2-43c0-c9ff-1f752dcf00c5"
      },
      "source": [
        "# Build ThunderSVM\n",
        "! cd thundersvm && mkdir build && cd build && cmake .. && make -j\n",
        "! python /content/thundersvm/python/setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘build’: File exists\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing thundersvm.egg-info/PKG-INFO\n",
            "writing dependency_links to thundersvm.egg-info/dependency_links.txt\n",
            "writing requirements to thundersvm.egg-info/requires.txt\n",
            "writing top-level names to thundersvm.egg-info/top_level.txt\n",
            "package init file 'thundersvm/__init__.py' not found (or not a regular file)\n",
            "reading manifest file 'thundersvm.egg-info/SOURCES.txt'\n",
            "writing manifest file 'thundersvm.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying thundersvm.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying thundersvm.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying thundersvm.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying thundersvm.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying thundersvm.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/thundersvm-0.3.4-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing thundersvm-0.3.4-py3.7.egg\n",
            "Removing /usr/local/lib/python3.7/dist-packages/thundersvm-0.3.4-py3.7.egg\n",
            "Copying thundersvm-0.3.4-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "thundersvm 0.3.4 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/thundersvm-0.3.4-py3.7.egg\n",
            "Processing dependencies for thundersvm==0.3.4\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for joblib==1.0.1\n",
            "Best match: joblib 1.0.1\n",
            "Adding joblib 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for thundersvm==0.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyurK6dcxTvD"
      },
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "thundersvm = SourceFileLoader(\"thundersvm\", \"/content/thundersvm/python/thundersvm/thundersvm.py\").load_module()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aegvi-FLowkc",
        "outputId": "326dc0e1-7fe4-4498-aeb6-3ce8631a0b5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JaJUjsIsgi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832e14a6-b18a-4113-cdc7-5c73aa5107d1"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from thundersvm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-a3FRpQtH4z"
      },
      "source": [
        "def process_csv(csv):\n",
        "    df = pd.read_csv(csv)\n",
        "    #df=df.drop(columns=['Case','lemma', 'Gender','Number','State','Gloss','Proclitic 1','Proclitic 2','Proclitic 0','Aspect','Mood','Person','Voice','Enclitic 0'])\n",
        "    df=df.drop_duplicates()\n",
        "    df[\"POS\"] = df[\"POS\"].apply(lambda x: x.replace(\"Proper Noun\", \"ProperNoun\"))\n",
        "    df1 = pd.DataFrame(columns = ['x','y'])\n",
        "    for i in df['sentence_id'].unique():\n",
        "        df1 = df1.append({'x':df[df['sentence_id'] == i]['word'].tolist() , 'y':df[df['sentence_id'] == i]['POS'].tolist()}, ignore_index=True)\n",
        "    return df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV87nSmBtgdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7be04b-afd1-4ccb-cfba-37a4ba19c1ba"
      },
      "source": [
        "csvs = os.listdir('/content/drive/MyDrive/POS_Arabic_data/Dataset/')\n",
        "final_df = pd.DataFrame()\n",
        "for csv in csvs:\n",
        "    df = process_csv('/content/drive/MyDrive/POS_Arabic_data/Dataset/'+csv)\n",
        "    final_df = pd.concat([final_df,df])\n",
        "final_df = final_df.sample(frac = 1)\n",
        "final_df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBpbBPB4tg8W"
      },
      "source": [
        "WINDOW = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxG6e9WV3ZIv"
      },
      "source": [
        "sentences = final_df['x'].tolist()\n",
        "poss = final_df['y'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrxcq_KuthNM"
      },
      "source": [
        "for i in range(len(sentences)):\n",
        "    sentences[i] = [\"\" for i in range(int(WINDOW)//2)] + sentences[i] + [\"\" for i in range(int(WINDOW)//2)]\n",
        "    poss[i] = [\"PAD\" for i in range(int(WINDOW)//2)] + poss[i] + [\"PAD\" for i in range(int(WINDOW)//2)]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS9P2gPGthZV"
      },
      "source": [
        "train_sents = sentences[:int(len(sentences)*0.7)]\n",
        "val_sents = sentences[int(len(sentences)*0.7):int(len(sentences)*0.9)]\n",
        "test_sents = sentences[int(len(sentences)*0.9):]\n",
        "assert len(train_sents)+len(val_sents)+len(test_sents) == len(sentences)\n",
        "train_pos = poss[:int(len(poss)*0.7)]\n",
        "val_pos = poss[int(len(poss)*0.7):int(len(poss)*0.9)]\n",
        "test_pos = poss[int(len(poss)*0.9):]\n",
        "assert len(train_pos)+len(val_pos)+len(test_pos) == len(poss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOPu9ri7thil"
      },
      "source": [
        "sents_train_val = train_sents.copy()\n",
        "sents_train_val.extend(val_sents)\n",
        "pos_train_val = train_pos.copy()\n",
        "pos_train_val.extend(val_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qik-vub5thrK"
      },
      "source": [
        "tags_list = []\n",
        "for tags in pos_train_val:\n",
        "    for tag in tags:\n",
        "        if tag not in tags_list:\n",
        "            tags_list.append(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeU5hEe-tvW4"
      },
      "source": [
        "words_list = []\n",
        "for words in sents_train_val:\n",
        "    for word in words:\n",
        "        if word not in words_list:\n",
        "            words_list.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiRT5AD-tvy3"
      },
      "source": [
        "word_freq = {}\n",
        "word_tags = {}\n",
        "for i in words_list:\n",
        "    tags = {}\n",
        "    for j in tags_list:\n",
        "        tags[j] = 0\n",
        "    word_tags[i] = tags\n",
        "    word_freq[i] = 0\n",
        "for i in range(len(sents_train_val)):\n",
        "    for j in range(len(sents_train_val[i])):\n",
        "        word_freq[sents_train_val[i][j]] += 1\n",
        "        word_tags[sents_train_val[i][j]][pos_train_val[i][j]] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT4K0xEatv5w"
      },
      "source": [
        "ambiguities = {}\n",
        "for word in words_list:\n",
        "    ambiguities[word] = [j for j in word_tags[word] if word_tags[word][j] !=0]\n",
        "    assert word_freq[word] == sum(word_tags[word].values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk6WPKPotwBb"
      },
      "source": [
        "embedding_model = gensim.models.Word2Vec.load('/content/drive/MyDrive/POS_Arabic_data/full_grams_cbow_100_twitter.mdl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyUOWAExtwOk"
      },
      "source": [
        "def OneHotEncoder(number, lenght):\n",
        "    zero = np.zeros(lenght)\n",
        "    zero[number] = 1\n",
        "    return zero\n",
        "\n",
        "def getFeatures(wordIdx, sentence, pos, w2v, tags, ambiguities, train=True):\n",
        "    features = []\n",
        "    \n",
        "    keys = w2v.wv.key_to_index.keys()\n",
        "    for i in reversed(range(1,int(WINDOW)//2+1)):\n",
        "        if sentence[wordIdx-i] not in keys:\n",
        "            features.append(np.zeros(w2v.vector_size))\n",
        "        else:\n",
        "            features.append(w2v.wv.get_vector(sentence[wordIdx-i], norm=True))\n",
        "    \n",
        "    if sentence[wordIdx] not in keys:\n",
        "        features.append(np.zeros(w2v.vector_size))\n",
        "    else:\n",
        "        features.append(w2v.wv.get_vector(sentence[wordIdx], norm=True))\n",
        "    \n",
        "    for i in range(1,int(WINDOW)//2+1):\n",
        "        if sentence[wordIdx+i] not in keys:\n",
        "            features.append(np.zeros(w2v.vector_size))\n",
        "        else:\n",
        "            features.append(w2v.wv.get_vector(sentence[wordIdx+i], norm=True))\n",
        "    if train:\n",
        "        for i in reversed(range(1,int(WINDOW)//2+1)):\n",
        "            tag = pos[wordIdx-i]\n",
        "            features.append(OneHotEncoder(tags.index(tag),len(tags)))\n",
        "            \n",
        "        if sentence[wordIdx] in ambiguities:\n",
        "            features.append(OneHotEncoder([tags.index(i) for i in ambiguities[sentence[wordIdx]]],len(tags)))\n",
        "        else:\n",
        "            features.append(OneHotEncoder([],len(tags)))\n",
        "\n",
        "    else:\n",
        "        for i in reversed(range(1,int(WINDOW)//2+1)):\n",
        "            tag = pos[wordIdx-i]\n",
        "            features.append(OneHotEncoder(tags.index(tag),len(tags)))\n",
        "            \n",
        "        if sentence[wordIdx] in ambiguities:\n",
        "            features.append(OneHotEncoder([tags.index(i) for i in ambiguities[sentence[wordIdx]]],len(tags)))\n",
        "        else:\n",
        "            features.append(OneHotEncoder([],len(tags)))\n",
        "    \n",
        "    features.append([len(sentence[wordIdx])])\n",
        "    \n",
        "    flat_list = []\n",
        "    for i in features:\n",
        "        flat_list.extend(i)\n",
        "    return flat_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYgYAbBuBgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bf7db8-128e-4b5c-931f-2ed06ba7d0bb"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "x_train = []\n",
        "y_train = []\n",
        "for i in tqdm(range(len(train_sents))):\n",
        "    for j in range(int(WINDOW)//2,len(train_sents[i]) - int(WINDOW)//2):\n",
        "        x_train.append(getFeatures(j , train_sents[i], train_pos[i], embedding_model, tags_list, ambiguities,train=True))\n",
        "        y_train.append(train_pos[i][j])\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "for i in tqdm(range(len(val_sents))):\n",
        "    for j in range(int(WINDOW)//2,len(val_sents[i]) - int(WINDOW)//2):\n",
        "        x_val.append(getFeatures(j , val_sents[i], val_pos[i], embedding_model, tags_list, ambiguities))\n",
        "        y_val.append(val_pos[i][j])\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "y_val = encoder.transform(y_val)\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "for i in tqdm(range(len(test_sents))):\n",
        "    for j in range(int(WINDOW)//2,len(test_sents[i]) - int(WINDOW)//2):\n",
        "        x_test.append(getFeatures(j , test_sents[i], test_pos[i], embedding_model, tags_list, ambiguities))\n",
        "        y_test.append(test_pos[i][j])\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "y_test = encoder.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14213/14213 [00:20<00:00, 691.06it/s]\n",
            "100%|██████████| 4061/4061 [00:05<00:00, 748.15it/s]\n",
            "100%|██████████| 2031/2031 [00:02<00:00, 702.40it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmElG3F2uBut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b817ab8-8677-46a4-bb00-29e6e1fac68e"
      },
      "source": [
        "print(\"Training data shape (x y) \", x_train.shape,y_train.shape)\n",
        "print(\"Validation data shape (x y) \", x_val.shape,y_val.shape)\n",
        "print(\"Test data shape (x y) \", x_test.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape (x y)  (273482, 603) (273482,)\n",
            "Validation data shape (x y)  (79245, 603) (79245,)\n",
            "Test data shape (x y)  (38991, 603) (38991,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHh84JtPuB4Q"
      },
      "source": [
        "clf = SVC(C=1, kernel='rbf')\n",
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FTqzxDttYKC",
        "outputId": "dad99227-67b3-4839-ed73-3e06d7c42a1b"
      },
      "source": [
        "train_pred = clf.predict(x_train)\n",
        "train_acc = sum(train_pred == y_train) *1.0 / len(train_pred)\n",
        "print('Train Acc',train_acc*100)\n",
        "train_f1 = f1_score(y_train, train_pred, average='macro')\n",
        "print('Train F1',train_f1*100)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Acc 97.65505590861557\n",
            "Train F1 69.06892239893415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjTAXscwmew",
        "outputId": "6ee10a0e-5a6a-4f66-b0f1-114f1fbcb349"
      },
      "source": [
        "val_pred = clf.predict(x_val)\n",
        "val_acc = sum(val_pred == y_val) *1.0 / len(val_pred)\n",
        "print('Val Acc',val_acc*100)\n",
        "val_f1 = f1_score(y_val, val_pred, average='macro')\n",
        "print('Val F1',val_f1*100)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val Acc 97.75001577386587\n",
            "Val F1 69.02655881650682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAItuH4gwo67",
        "outputId": "dfb33c24-92ba-4d5c-fb95-fe108522b260"
      },
      "source": [
        "test_pred = clf.predict(x_test)\n",
        "test_acc = sum(test_pred == y_test) *1.0 / len(test_pred)\n",
        "print('Test Acc',test_acc*100)\n",
        "test_f1 = f1_score(y_test, test_pred, average='macro')\n",
        "print('Test F1',test_f1*100)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Acc 93.66520479084916\n",
            "Test F1 68.36791269584921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoJNr9ulwpk_"
      },
      "source": [
        "def pos_tagger(w, sent):\n",
        "    words = sent.split()\n",
        "    words = [\"\" for i in range(int(w)//2)] + words + [\"\" for i in range(int(w)//2)]\n",
        "    pos = [\"PAD\" for i in range(int(w)//2)]\n",
        "    for i in range(int(WINDOW)//2,len(words) - int(WINDOW)//2):\n",
        "        feature = np.array(getFeatures(i , words, pos, embedding_model, tags_list, ambiguities, train = False)).reshape(1,-1)\n",
        "        tag = clf.predict(feature)\n",
        "        tag = encoder.inverse_transform([int(tag[0])])\n",
        "        pos.append(tag[0])\n",
        "    return pos[2:]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ47qDvBwsdn",
        "outputId": "f9766130-b0ed-4e25-cb3b-ea8f32f30ea6"
      },
      "source": [
        "sentence = \"جون يحب البيت الأزرق في نهاية الشارع\"\n",
        "output = pos_tagger(WINDOW ,sentence)\n",
        "pred_tags = [(sentence.split()[i],output[i]) for i in range(len(sentence.split()))]\n",
        "for w,t in pred_tags:\n",
        "    print(w,t)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "جون ProperNoun\n",
            "يحب Verb\n",
            "البيت Noun\n",
            "الأزرق Noun\n",
            "في Preposition\n",
            "نهاية Noun\n",
            "الشارع Noun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MLu9286z3Yo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}