{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1f1e76-aab8-420a-a32b-5e7793b75aef",
   "metadata": {},
   "source": [
    "<h1>Task 8: Word Embedding</h1>\n",
    "\n",
    "<h4> This notebook compares different embedding methods on a simple task (sentiment analysis) <a href=\"https://www.kaggle.com/mksaad/arabic-sentiment-twitter-corpus\">on a small dataset</a>.</h4>\n",
    "\n",
    "<h4>Table of Contents:</h4>\n",
    "<ol>\n",
    "    <li>Load Dataset</li>\n",
    "    <li>Normalize Dataset</li>\n",
    "    <li>Tokenize Dataset</li>\n",
    "    <li>Word Embedding</li>\n",
    "    <li>Train RNN model</li>\n",
    "    <li>Evaluate model</li>\n",
    "</ol>\n",
    "<h4>Embedding Methods:</h4>\n",
    "<ol>\n",
    "    <li>Keras Embedding Layer (trained from scratch)</li>\n",
    "    <li>Keras Word2Vec implementation (trained from scratch)</li>\n",
    "    <li>Genism library's Word2Vec implementation (trained from scratch)</li>\n",
    "    <li>Genism library's GloVe implementation (trained from scratch)</li>\n",
    "    <li>Genism library's fasttext implementation (trained from scratch)</li>\n",
    "    <li>AraVec pretrained embeddings</li>\n",
    "    <li>Arabic-Chapter pretrained embeddings</li>\n",
    "    <li>BERT Arabic pretrained model</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cd5e5-08d7-4614-97cf-90e86c805069",
   "metadata": {},
   "source": [
    "<h1>Load Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0753d856-c632-4660-b903-512d7385b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_pos = pd.read_csv(\"data/train_Arabic_tweets_positive_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "train_neg = pd.read_csv(\"data/train_Arabic_tweets_negative_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "test_pos = pd.read_csv(\"data/test_Arabic_tweets_positive_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "test_neg = pd.read_csv(\"data/test_Arabic_tweets_negative_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "train = pd.concat([train_pos, train_neg])#.sample(frac=1, random_state=0)\n",
    "test = pd.concat([test_pos, test_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf0ac73-68f4-4f33-ad8a-49e161e566dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>من الخير نفسه 💛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>neg</td>\n",
       "      <td>كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>neg</td>\n",
       "      <td>احسدك على الايم 💔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>neg</td>\n",
       "      <td>لأول مرة ما بنكون سوا 💔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>neg</td>\n",
       "      <td>بقله ليش يا واطي 🤔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>neg</td>\n",
       "      <td>قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0       pos  نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n",
       "1       pos  وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n",
       "2       pos                                    من الخير نفسه 💛\n",
       "3       pos  #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n",
       "4       pos  الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n",
       "...     ...                                                ...\n",
       "22509   neg  كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...\n",
       "22510   neg                                  احسدك على الايم 💔\n",
       "22511   neg                            لأول مرة ما بنكون سوا 💔\n",
       "22512   neg                                 بقله ليش يا واطي 🤔\n",
       "22513   neg  قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...\n",
       "\n",
       "[45275 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec34d35c-034b-4d11-8ea6-6767653d47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize(text):\n",
    "    text = araby.strip_harakat(text)\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    text = araby.strip_small(text)\n",
    "    text = araby.strip_tatweel(text)\n",
    "    text = araby.strip_shadda(text)\n",
    "    text = araby.strip_diacritics(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    #text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    return text\n",
    "\n",
    "def strip_all(text):\n",
    "    l = [' ', '0', '1', '2', '3', '4', '5', '6',\n",
    "       '7', '8', '9', '?', \n",
    "       '؟', 'ء', 'ؤ', 'ئ', 'ا', 'ب', 'ت', 'ث',\n",
    "       'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ',\n",
    "       'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', '٠', '١',\n",
    "       '٢', '٣', '٤', '٥', '٦', '٧', '٨', '٩']\n",
    "    return \"\".join([x for x in text if x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea115c1-fddf-49f7-ba66-db45bff6c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "train.tweet = train.tweet.apply(normalize).apply(strip_all).apply(araby.tokenize)\n",
    "test.tweet = test.tweet.apply(normalize).apply(strip_all).apply(araby.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b6b7fb-6f01-4b77-9334-65fd3d346dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train.label)\n",
    "train.label = le.transform(train.label)\n",
    "test.label = le.transform(test.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9c3e9a-66e2-45fa-915e-2475f3785d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[نحن, الذين, يتحول, كل, ما, نود, ان, نقوله, ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[وفي, النهايه, لن, يبقا, معك, احدالا, من, راا,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[من, الخير, نفسه]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[زلزلالملعبنصرنابيلعب, كن, عالي, الهمه, ولا, ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[الشيء, الوحيد, الذي, وصلوا, فيه, للعالميه, هو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>0</td>\n",
       "      <td>[كيف, ترا, اورانوس, لو, كان, يقع, مكان, القمر,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>0</td>\n",
       "      <td>[احسدك, علا, الايم]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>0</td>\n",
       "      <td>[لاول, مره, ما, بنكون, سوا]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>0</td>\n",
       "      <td>[بقله, ليش, يا, واطي]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>0</td>\n",
       "      <td>[قد, طال, صبري, في, النوا, اذ, تركتني, كئيبا, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "0          1  [نحن, الذين, يتحول, كل, ما, نود, ان, نقوله, ال...\n",
       "1          1  [وفي, النهايه, لن, يبقا, معك, احدالا, من, راا,...\n",
       "2          1                                  [من, الخير, نفسه]\n",
       "3          1  [زلزلالملعبنصرنابيلعب, كن, عالي, الهمه, ولا, ت...\n",
       "4          1  [الشيء, الوحيد, الذي, وصلوا, فيه, للعالميه, هو...\n",
       "...      ...                                                ...\n",
       "22509      0  [كيف, ترا, اورانوس, لو, كان, يقع, مكان, القمر,...\n",
       "22510      0                                [احسدك, علا, الايم]\n",
       "22511      0                        [لاول, مره, ما, بنكون, سوا]\n",
       "22512      0                              [بقله, ليش, يا, واطي]\n",
       "22513      0  [قد, طال, صبري, في, النوا, اذ, تركتني, كئيبا, ...\n",
       "\n",
       "[45275 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3ff382-6aca-414a-936b-2f93adcfd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train.tweet.values, train.label.values, test_size=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01b5856-3e86-4d12-bb77-779ffbce575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding import WordEmbedding\n",
    "from utils import helper, preprocess\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3d4b35-a57a-4840-8738-5d85bcf50e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42394/42394 [00:00<00:00, 44189.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5473)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                54740     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5473)              60203     \n",
      "=================================================================\n",
      "Total params: 114,943\n",
      "Trainable params: 114,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1325/1325 [==============================] - 6s 4ms/step - loss: 0.2986 - accuracy: 0.0081\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.11875, saving model to models/word_embeddings.h5\n",
      "Epoch 2/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0038 - accuracy: 0.0214\n",
      "\n",
      "Epoch 00002: loss improved from 0.11875 to 0.00305, saving model to models/word_embeddings.h5\n",
      "Epoch 3/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0020 - accuracy: 0.0224\n",
      "\n",
      "Epoch 00003: loss improved from 0.00305 to 0.00187, saving model to models/word_embeddings.h5\n",
      "Epoch 4/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0017 - accuracy: 0.0214\n",
      "\n",
      "Epoch 00004: loss improved from 0.00187 to 0.00168, saving model to models/word_embeddings.h5\n",
      "Epoch 5/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0016 - accuracy: 0.0209\n",
      "\n",
      "Epoch 00005: loss improved from 0.00168 to 0.00163, saving model to models/word_embeddings.h5\n"
     ]
    }
   ],
   "source": [
    "# Word2vec\n",
    "embeddings = WordEmbedding(preprocess.tokenizer, vocab_size=13000, maxlen=150, embedding_vector=10, method=\"word2vec\")\n",
    "#text = embeddings.tokenize(text) We already did tokenization\n",
    "words, label, unique_words, word_dict = embeddings.encode_w2v(train.tweet.values[:1000]) #Consumes very large amount of memory\n",
    "model = embeddings.train_w2v(words, label, epochs=5)\n",
    "\n",
    "word_embeddings = model.get_weights()[0]\n",
    "\n",
    "# embeddings = helper.get_embeddings(unique_words, word_dict, word_embeddings)\n",
    "# helper.plot(word_dict, embeddings)\n",
    "# helper.save_embeddings(embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b6ba0efc-2a73-480a-ba4f-92839c6c0ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606585, 652324)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "sentences = np.concatenate([train.tweet.values, test.tweet.values])\n",
    "word_model = gensim.models.Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "word_model.train(sentences, total_examples=word_model.corpus_count, epochs=1)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cfa19dd1-9aa3-4981-9625-45cdf304610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word_model.syn1neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c926f74-64d0-4d68-9774-f28fa11bcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.key_to_index[word]\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index_to_key[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3ed6f2e-e8d2-4b25-ac8d-3a786b7b4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tmp = np.zeros([X_train.shape[0], 150], dtype=np.int32)\n",
    "cnt,cntt=0,0\n",
    "for i, sentence in enumerate(X_train):\n",
    "    for t, word in enumerate(sentence[:150]):\n",
    "        if word in word_model.wv.key_to_index:\n",
    "            X_train_tmp[i, t] = word2idx(word)\n",
    "            cntt += 1\n",
    "        else:\n",
    "            X_train_tmp[i, t] = 0\n",
    "            cnt += 1\n",
    "X_train = X_train_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32925da2-ce69-436e-a1e9-bae694ee02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_tmp = np.zeros([X_valid.shape[0], 150], dtype=np.int32)\n",
    "cnt,cntt=0,0\n",
    "for i, sentence in enumerate(X_valid):\n",
    "    for t, word in enumerate(sentence[:150]):\n",
    "        if word in word_model.wv.key_to_index:\n",
    "            X_valid_tmp[i, t] = word2idx(word)\n",
    "            cntt += 1\n",
    "        else:\n",
    "            X_valid_tmp[i, t] = 0\n",
    "            cnt += 1\n",
    "X_valid = X_valid_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f200cc9d-35bf-43fb-85e2-5e93bb5c9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size = weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "03fa6ad9-bab8-4d50-9323-5f08bda343e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from random import shuffle\n",
    "from pyarabic import araby\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional, BatchNormalization, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1991ac9c-02ff-42db-b9ed-f294c134ee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  145,     6,  5148, ...,     0,     0,     0],\n",
       "       [   10,   274,     4, ...,     0,     0,     0],\n",
       "       [57783,     1, 57811, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 4286, 36334,   136, ...,     0,     0,     0],\n",
       "       [   18, 33922,  4851, ...,     0,     0,     0],\n",
       "       [   89,    70,    19, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be3d713b-e4b7-494f-8e1a-fd71662dfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((150,)))\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[weights]))\n",
    "model.add(Bidirectional(GRU(units = 32, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(units = 32, return_sequences=False)))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b7ebf0a-b447-4397-9f0a-3575e7f53ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "177/177 [==============================] - 19s 81ms/step - loss: 0.6368 - accuracy: 0.6118 - val_loss: 0.4767 - val_accuracy: 0.7558\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.75576, saving model to full_verse_7.h5\n",
      "Epoch 2/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.3280 - accuracy: 0.8634 - val_loss: 0.5353 - val_accuracy: 0.7563\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.75576 to 0.75634, saving model to full_verse_7.h5\n",
      "Epoch 3/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.1375 - accuracy: 0.9514 - val_loss: 0.6196 - val_accuracy: 0.7501\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.75634\n",
      "Epoch 4/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0812 - accuracy: 0.9720 - val_loss: 0.7676 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.75634 to 0.75974, saving model to full_verse_7.h5\n",
      "Epoch 5/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.8371 - val_accuracy: 0.7566\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.75974\n",
      "Epoch 6/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0526 - accuracy: 0.9796 - val_loss: 0.8895 - val_accuracy: 0.7570\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.75974\n",
      "Epoch 7/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0511 - accuracy: 0.9804 - val_loss: 0.9423 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.75974\n",
      "Epoch 8/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0450 - accuracy: 0.9821 - val_loss: 0.9752 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.75974\n",
      "Epoch 9/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0435 - accuracy: 0.9821 - val_loss: 1.0026 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.75974\n",
      "Epoch 10/15\n",
      "177/177 [==============================] - 14s 76ms/step - loss: 0.0384 - accuracy: 0.9846 - val_loss: 1.0448 - val_accuracy: 0.7572\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.75974\n",
      "Epoch 11/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0380 - accuracy: 0.9845 - val_loss: 1.0802 - val_accuracy: 0.7577\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.75974\n",
      "Epoch 12/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0372 - accuracy: 0.9830 - val_loss: 1.1148 - val_accuracy: 0.7583\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.75974\n",
      "Epoch 13/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0363 - accuracy: 0.9834 - val_loss: 1.1448 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.75974\n",
      "Epoch 14/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0340 - accuracy: 0.9846 - val_loss: 1.1646 - val_accuracy: 0.7553\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.75974\n",
      "Epoch 15/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0345 - accuracy: 0.9840 - val_loss: 1.2084 - val_accuracy: 0.7579\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.75974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae44406520>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.0001, min_lr=0.0001)]\n",
    "callbacks += [tf.keras.callbacks.ModelCheckpoint('gensim_w2v_scratch.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')]\n",
    "model.fit(X_train, y_train, validation_data= (X_valid, y_valid), epochs = 15, batch_size= 128, shuffle = True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c80b8-9385-4cc7-ac2d-7d248cad384a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
